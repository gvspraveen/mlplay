{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q learning based solution for [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0)\n",
    "\n",
    "Mostly based on explanations in [mnemstudio tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm) and this wonderful RL series in [Medium](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0).\n",
    "\n",
    "### Uses Numpy\n",
    "This is a very basic implementation. Does not perform very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Open AI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-21 15:47:51,668] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Gym parameters\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_reduction = 0.1\n",
    "gamma_reduction = 0.9\n",
    "num_episodes = 100000\n",
    "\n",
    "# print the reward for episode every n episodes\n",
    "print_every = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Q array with zeros\n",
    "Q = np.zeros([state_size, action_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play():\n",
    "    total_reward = 0\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get start state\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        count = 0\n",
    "        # Algorithm works by taking a step at a time based on Q learning. Gym tells us at each step if we reached `done`\n",
    "        # state. Done state means either we failed or reach goal state. But it could be possible that our algorithm\n",
    "        # keeps going in circles and never reaches done state. We should break this.\n",
    "        # In my approach I assume that if our step count is twice size of the board that means we are not getting anywhere.\n",
    "        break_loop_at = state_size * action_size * 2\n",
    "        while count < break_loop_at and not done:\n",
    "            count += 1\n",
    "            \n",
    "            # Look at current Q table and pick the action with maximum reward greedily. But according to the requirements\n",
    "            # given in Game, there could be wind (read as noise) which could sway our decision. So we simulate noise by\n",
    "            # adding random values to all actions in this state and then picking the max action (greedy)\n",
    "            action_with_noise = Q[state, :] + np.random.randn(1, action_size)*(1.0/(count+1))\n",
    "            action = np.argmax(action_with_noise)\n",
    "            # Make the step and get the next_state and reward from env\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "\n",
    "            # Update Q table.\n",
    "            Q[state, action] = q_reduction * Q[state, action] + (1-q_reduction)*(reward + gamma_reduction * np.max(Q[next_state:]))\n",
    "            \n",
    "            # Upadate episode reward\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update the state\n",
    "            state = next_state\n",
    "       \n",
    "        total_reward += episode_reward\n",
    "        if i % print_every == 0:\n",
    "            print \"Episode: {}, Total Reward: {}\".format(i, \"{0:.4f}\".format(total_reward/(i+1)))\n",
    "\n",
    "    print \"================== Reward ================\"\n",
    "    print \"Total Reward : {} \".format(\"{0:.4f}\".format(total_reward/num_episodes))\n",
    "    print \"================== Q =================\"    \n",
    "    print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 0.0000\n",
      "Episode: 10000, Total Reward: 0.0069\n",
      "Episode: 20000, Total Reward: 0.0096\n",
      "Episode: 30000, Total Reward: 0.0118\n",
      "Episode: 40000, Total Reward: 0.0124\n",
      "Episode: 50000, Total Reward: 0.0129\n",
      "Episode: 60000, Total Reward: 0.0131\n",
      "Episode: 70000, Total Reward: 0.0137\n",
      "Episode: 80000, Total Reward: 0.0139\n",
      "Episode: 90000, Total Reward: 0.0146\n",
      "================== Reward ================\n",
      "Total Reward : 0.0150 \n",
      "================== Q =================\n",
      "[[ 0.73374625  0.7337464   0.73374777  0.7337464 ]\n",
      " [ 0.7337464   0.73374634  0.73374849  0.73382034]\n",
      " [ 0.73382034  0.74101357  0.74100334  0.74190052]\n",
      " [ 0.          0.73588472  0.          0.74190052]\n",
      " [ 0.72712984  0.7277258   0.72713044  0.7337536 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.73601705  0.757301    0.          0.81527361]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.72736642  0.68914406  0.72857276  0.72766779]\n",
      " [ 0.73441483  0.66105629  0.68840136  0.        ]\n",
      " [ 0.74695701  0.          0.66854626  0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.72589518  0.          0.          0.68253577]\n",
      " [ 0.          0.          0.75014386  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
