{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q learning based solution for [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0)\n",
    "\n",
    "Mostly based on explanations in [mnemstudio tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm) and this wonderful RL series in [Medium](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0).\n",
    "\n",
    "### With a simple neural net using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Open AI gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 17:32:21,824] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Gym parameters\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma_reduction = 0.99\n",
    "num_episodes = 10000\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# print the reward for episode every n episodes\n",
    "print_every = 1000\n",
    "\n",
    "max_loops_per_episode = state_size * action_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Tensors\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# hidden_layer_size = 10\n",
    "# W1Size = [state_size, hidden_layer_size]\n",
    "# W2Size = [hidden_layer_size, action_size]\n",
    "\n",
    "# bais1 = tf.Variable(tf.random_normal([hidden_layer_size]))\n",
    "# bias2 = tf.Variable(tf.random_normal([action_size]))\n",
    "\n",
    "# Win = tf.Variable(tf.random_uniform(W1Size, 0, 0.01))\n",
    "# WOut = tf.Variable(tf.random_uniform(W2Size, 0, 0.01))\n",
    "\n",
    "# layer1 = tf.add(tf.matmul(inputs, Win), bais1)\n",
    "# Qout = tf.add(tf.matmul(layer1, WOut), bias2)\n",
    "\n",
    "inputs = tf.placeholder(shape=[1,state_size],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([state_size,action_size],0,0.01))\n",
    "# bias2 = tf.Variable(tf.random_normal([action_size]))\n",
    "Qout = tf.matmul(inputs,W)\n",
    "predicted_move = tf.argmax(Qout,1)\n",
    "\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,action_size],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play(print_every, max_loops_per_episode):\n",
    "    \n",
    "    rand_action_prob = 0.1\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Training logic\n",
    "    rewards = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(num_episodes):\n",
    "            # Get the starting state\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            # Algorithm works by taking a step at a time based on Q learning. Gym tells us at each step if we reached `done`\n",
    "            # state. Done state means either we failed or reach goal state. But it could be possible that our algorithm\n",
    "            # keeps going in circles and never reaches done state. We should break this.\n",
    "            # In my approach I assume that if our step count is twice size of the board that means we are not getting anywhere.\n",
    "            count = 0\n",
    "            while count < max_loops_per_episode and not done:\n",
    "                count += 1\n",
    "                next_action, predictedQ = sess.run([predicted_move,Qout], feed_dict={inputs:np.identity(16)[state:state+1]})\n",
    "                \n",
    "                    # According to the specifications we have to take into account random wind which might force us into a\n",
    "    # arbitrary action\n",
    "                if np.random.rand(1) < rand_action_prob:\n",
    "                    next_action[0] = env.action_space.sample()\n",
    "                \n",
    "                # Get new state and reward by taking the `predicted_move`\n",
    "                new_state, reward, done, _ = env.step(next_action[0])\n",
    "\n",
    "                # Get Q values for next state\n",
    "                nextStateQ = sess.run(Qout, feed_dict={inputs:np.identity(16)[new_state:new_state+1]})\n",
    "                maxNextState = np.max(nextStateQ)\n",
    "                updatedQ = predictedQ\n",
    "                updatedQ[0, next_action[0]] = reward + gamma_reduction * maxNextState\n",
    "                _, _ = sess.run([updateModel, W] ,feed_dict={inputs:np.identity(16)[state:state+1],nextQ:updatedQ})\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = new_state\n",
    "                \n",
    "                if done == True:\n",
    "                    #Reduce chance of random action because of wind as we train the model.\n",
    "                    rand_action_prob = 1./((i/50) + 10)\n",
    "                    break                \n",
    "                \n",
    "            rewards.append(episode_reward)\n",
    "            if i % print_every == 0:\n",
    "                print \"Episode: {}, Total Reward: {}\".format(i, str(sum(rewards)/(i+1)))\n",
    "        \n",
    "        print \"================== Reward ================\"\n",
    "        print \"Total Reward : \"+ str(sum(rewards)/num_episodes)\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 1.0\n",
      "Episode: 1000, Total Reward: 0.0769230769231\n",
      "Episode: 2000, Total Reward: 0.0764617691154\n",
      "Episode: 3000, Total Reward: 0.0779740086638\n",
      "Episode: 4000, Total Reward: 0.0817295676081\n",
      "Episode: 5000, Total Reward: 0.0817836432713\n",
      "Episode: 6000, Total Reward: 0.0849858356941\n",
      "Episode: 7000, Total Reward: 0.0879874303671\n",
      "Episode: 8000, Total Reward: 0.0951131108611\n",
      "Episode: 9000, Total Reward: 0.101322075325\n",
      "================== Reward ================\n",
      "Total Reward : 0.1059\n"
     ]
    }
   ],
   "source": [
    "rewards = play(print_every, max_loops_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11417ed90>]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEttJREFUeJzt3X2QXXV9x/H3N7vZhISEJOwGQx5IkESNDk9uEUUpg6hA\nbWinVsno+CwzKh2fxgqDQyvtjLW21DJFgarVWuXBx6YYJyoito5glgeRJASX8JAEMAuBBAkhwXz7\nxz3EmyXJvbu54eb+eL9m7uSc3/nde7+/+1s+e/acczmRmUiSyjKm3QVIklrPcJekAhnuklQgw12S\nCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVqLtdb9zb25tz585t19tLUke6+eabH87Mvkb92hbuc+fO\nZWBgoF1vL0kdKSLua6afh2UkqUCGuyQVyHCXpAIZ7pJUIMNdkgrUMNwj4ssRsSEi7tjD9oiISyJi\nMCJuj4jjW1+mJGkkmtlz/wpw+l62nwHMrx7nAF/Y97IkSfui4XXumfmziJi7ly5nAf+Ztfv13RgR\nUyJiRmY+2KIad7H83o38711DjBvbxWeXrWZMwI7qToF/duzhfO+2Bzh5QR8vmDyOawbW8Yp507h9\n3SbeePQMpk3s4dxTj+LiH90FwG1rH+OkF/by+NbtfPUXtUtHj+ydyEObt7Jl2++f9d4nHXUo47u7\nOOyQ8Vy9fC3vPmkuG5/YznV3/pbHtmznT485nP/51QN85LQFvOCQcXzi278G4IaPn8K6R5/kn3+4\nmq3bd7Dh8adY+qFX86Yv/IL7N24BYPa0g/j0nx/Nqgc3s+qhzXznlvUAfO4tx7Lpye38/fdXcuqL\np7NsxW/51KKXsvKBzVw9sJbPveVYPnz1bUwaV5vK7Tt28PqFL+ChTVshIDOZPmk8Kx/czD0PP8HF\nbz6GVQ9u5ru3PsCYgG2/38GpL57OwL2Pcv/GLUyb2MPHXr+AC757BzOnHMT6x57ksrcdz46ED3z9\nll0+jwvOfAmDG37H9t/vYPa0CWx4fCtX/nItAL0Hj2Pz1u2861Vzufxna3jZzMncsX4zc6ZN4P6N\nWzh0Yg+PPLGNqRPGMrZrDBsef2qX1z7vjBcze+oEPviNWzhuzhTue2QLG5/YtkufD582nwk9XXz+\np3ez4LBJbHxiG/N6J3LDXUPMOGQ8k8eP5djZU7h6YC3bnt7BSw+fzDGzp/DU9h1MHNfFygc28+V3\n/RFf+fm9ANx836PccNcQi0+Yw2vm9/KBr9/CnGkTGHr8KQ7q6aKnawwPbd668/1fNnMyD23aysO/\n27Wul8yYzKoHN+9cP27OFA6fchDHzprC5358F0/U/Wy97cQ5/NeN99N7cM+zXuf9p7yQy264m/4j\nprL83kcBeN3CwxgTsGzFb3f2G/7cNx49g4k93fx6/SZWVnVMGt/NEYdO4JQF0/n53Q9z6/2PsTtH\nzzqE29dt4qjpBzOxp4tfrdu0234AE3q62LLt90wa183jTz0NQE/3GH744ZN56xdv4uQFvTt/HvZk\n/NgxbN2+A4DJ47s56ahefnDHQ3t9Tm3M48hMJh80lnsefmJn+4lHTuPGNRsBmD5p3LN+roZ7xbxp\n3HTPxp3rC2dM3vmZNXLIQWPZ9OT2PW5/1QsPZf70g3dmyzM++Scv4b2vObKp99gX0cw9VKtwvzYz\nX7abbdcC/5CZ/1etXwd8IjOf9Q2liDiH2t49c+bMefl99zV1Lf4uLr/hbj79gztH/DxJOlB85wOv\n4vg5U0f13Ii4OTP7G/V7Tk+oZuYVmdmfmf19fQ2/PStJRXpyN0cGWq0V4b4emF23PqtqkyS1SSvC\nfQnw9uqqmROBTfvreLskqTkNT6hGxJXAKUBvRKwD/gYYC5CZlwFLgTOBQWAL8K79VawkqTnNXC2z\nuMH2BD7YsookSfvMb6hKUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJ\nKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBeq4cI9odwWSdODruHCX\nJDVmuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoGaCveIOD0iVkfEYESc\nt5vtcyLi+oi4NSJuj4gzW1+qJKlZDcM9IrqAS4EzgIXA4ohYOKzbJ4FrMvM44Gzg860uVJLUvGb2\n3E8ABjNzTWZuA64CzhrWJ4HJ1fIhwAOtK1GSNFLdTfSZCaytW18HvGJYn78FfhgRfwVMBE5rSXWS\npFFp1QnVxcBXMnMWcCbwtYh41mtHxDkRMRARA0NDQy16a0nScM2E+3pgdt36rKqt3nuAawAy8xfA\neKB3+Atl5hWZ2Z+Z/X19faOrWJLUUDPhvhyYHxHzIqKH2gnTJcP63A+8FiAiXkIt3N01l6Q2aRju\nmfk0cC6wDFhF7aqYFRFxUUQsqrp9DHhfRPwKuBJ4Z2bm/ipakrR3zZxQJTOXAkuHtV1Yt7wSOKm1\npUmSRstvqEpSgQx3SSqQ4S5JBTLcJalAhrskFajjwj2IdpcgSQe8jgt3SVJjhrskFchwl6QCGe6S\nVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkF\nMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSpQx4V7RLsrkKQDX8eFuySpsabCPSJOj4jV\nETEYEeftoc+bI2JlRKyIiG+0tkxJ0kh0N+oQEV3ApcDrgHXA8ohYkpkr6/rMB84HTsrMRyNi+v4q\nWJLUWDN77icAg5m5JjO3AVcBZw3r8z7g0sx8FCAzN7S2TEnSSDQT7jOBtXXr66q2eguABRHx84i4\nMSJO390LRcQ5ETEQEQNDQ0Ojq1iS1FCrTqh2A/OBU4DFwL9HxJThnTLziszsz8z+vr6+Fr21JGm4\nZsJ9PTC7bn1W1VZvHbAkM7dn5j3AXdTCXpLUBs2E+3JgfkTMi4ge4GxgybA+36O2105E9FI7TLOm\nhXVKkkagYbhn5tPAucAyYBVwTWauiIiLImJR1W0Z8EhErASuBz6emY/sr6IlSXvX8FJIgMxcCiwd\n1nZh3XICH60ekqQ28xuqklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNd\nkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWp\nQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUBNhXtEnB4RqyNiMCLO20u/v4iI\njIj+1pUoSRqphuEeEV3ApcAZwEJgcUQs3E2/ScCHgJtaXaQkaWSa2XM/ARjMzDWZuQ24CjhrN/3+\nDvgMsLWF9UmSRqGZcJ8JrK1bX1e17RQRxwOzM/P7LaxNkjRK+3xCNSLGABcDH2ui7zkRMRARA0ND\nQ/v61pKkPWgm3NcDs+vWZ1Vtz5gEvAz4aUTcC5wILNndSdXMvCIz+zOzv6+vb/RVS5L2qplwXw7M\nj4h5EdEDnA0seWZjZm7KzN7MnJuZc4EbgUWZObBfKpYkNdQw3DPzaeBcYBmwCrgmM1dExEURsWh/\nFyhJGrnuZjpl5lJg6bC2C/fQ95R9L0uStC867huqEdHuEiTpgNdx4S5Jasxwl6QCGe6SVCDDXZIK\nZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCG\nuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAHRfu0e4CJKkDdFy4S5IaM9wlqUCG\nuyQVyHCXpAIZ7pJUoKbCPSJOj4jVETEYEeftZvtHI2JlRNweEddFxBGtL1WS1KyG4R4RXcClwBnA\nQmBxRCwc1u1WoD8zjwa+BfxjqwuVJDWvmT33E4DBzFyTmduAq4Cz6jtk5vWZuaVavRGY1doyJUkj\n0Uy4zwTW1q2vq9r25D3AD3a3ISLOiYiBiBgYGhpqvkpJ0oi09IRqRLwN6Ac+u7vtmXlFZvZnZn9f\nX18r31qSVKe7iT7rgdl167Oqtl1ExGnABcAfZ+ZTrSlPkjQazey5LwfmR8S8iOgBzgaW1HeIiOOA\ny4FFmbmh9WVKkkaiYbhn5tPAucAyYBVwTWauiIiLImJR1e2zwMHANyPitohYsoeXkyQ9B5o5LENm\nLgWWDmu7sG75tBbXJUnaB35DVZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12S\nCmS4S1KBOi7cI9pdgSQd+Dou3CVJjRnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCX\npAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIK1FS4R8TpEbE6\nIgYj4rzdbB8XEVdX22+KiLmtLlSS1LyG4R4RXcClwBnAQmBxRCwc1u09wKOZeRTwL8BnWl2oJKl5\nzey5nwAMZuaazNwGXAWcNazPWcBXq+VvAa+NiGhdmZKkkWgm3GcCa+vW11Vtu+2TmU8Dm4BDW1Gg\nJJXm+js37Pf3eE5PqEbEORExEBEDQ0NDo3qNU140nUXHHD7qGl4xb9qonztaMw4Z/6y2uYdOeM7r\n0LO99PDJ7S6hOL0H97S7hAPeyQv69vt7dDfRZz0wu259VtW2uz7rIqIbOAR4ZPgLZeYVwBUA/f39\nOZqC5/VO5JLFx3HJ4uNG83RJel5oZs99OTA/IuZFRA9wNrBkWJ8lwDuq5TcBP8nMUYW3JGnfNdxz\nz8ynI+JcYBnQBXw5M1dExEXAQGYuAb4EfC0iBoGN1H4BSJLapJnDMmTmUmDpsLYL65a3An/Z2tIk\nSaPlN1QlqUCGuyQVyHCXpAIZ7pJUIMNdkgoU7bocPSKGgPtG+fRe4OEWltMJHPPzg2N+ftiXMR+R\nmQ2/4tq2cN8XETGQmf3truO55JifHxzz88NzMWYPy0hSgQx3SSpQp4b7Fe0uoA0c8/ODY35+2O9j\n7shj7pKkvevUPXdJ0l50XLg3ull3p4iI2RFxfUSsjIgVEfGhqn1aRPwoIn5T/Tu1ao+IuKQa9+0R\ncXzda72j6v+biHjHnt7zQBERXRFxa0RcW63Pq26sPljdaL2nat/jjdcj4vyqfXVEvKE9I2lOREyJ\niG9FxJ0RsSoiXln6PEfER6qf6zsi4sqIGF/aPEfElyNiQ0TcUdfWsnmNiJdHxK+r51wSMcJbl2Zm\nxzyo/S+H7waOBHqAXwEL213XKMcyAzi+Wp4E3EXtBuT/CJxXtZ8HfKZaPhP4ARDAicBNVfs0YE31\n79RqeWq7x9dg7B8FvgFcW61fA5xdLV8GvL9a/gBwWbV8NnB1tbywmvtxwLzqZ6Kr3ePay3i/Cry3\nWu4BppQ8z9Ruu3kPcFDd/L6ztHkGTgaOB+6oa2vZvAK/rPpG9dwzRlRfuz+gEX6YrwSW1a2fD5zf\n7rpaNLb/Bl4HrAZmVG0zgNXV8uXA4rr+q6vti4HL69p36XegPajdyes64FTg2uoH92Gge/gcU7uH\nwCur5e6qXwyf9/p+B9qD2l3J7qE6vzV8/kqcZ/5wT+Vp1bxdC7yhxHkG5g4L95bMa7Xtzrr2Xfo1\n8+i0wzLN3Ky741R/hh4H3AQclpkPVpseAg6rlvc09k77TD4H/DWwo1o/FHgsazdWh13r39ON1ztp\nzPOAIeA/qkNRX4yIiRQ8z5m5Hvgn4H7gQWrzdjNlz/MzWjWvM6vl4e1N67RwL05EHAx8G/hwZm6u\n35a1X9nFXM4UEW8ENmTmze2u5TnUTe1P9y9k5nHAE9T+XN+pwHmeCpxF7Rfb4cBE4PS2FtUG7Z7X\nTgv3Zm7W3TEiYiy1YP96Zn6nav5tRMyots8ANlTtexp7J30mJwGLIuJe4Cpqh2b+FZgStRurw671\n7xxb7Hrj9U4a8zpgXWbeVK1/i1rYlzzPpwH3ZOZQZm4HvkNt7kue52e0al7XV8vD25vWaeHezM26\nO0J15vtLwKrMvLhuU/3Nxt9B7Vj8M+1vr866nwhsqv78Wwa8PiKmVntMr6/aDjiZeX5mzsrMudTm\n7ieZ+Vbgemo3Vodnj3l3N15fApxdXWUxD5hP7eTTASczHwLWRsSLqqbXAispeJ6pHY45MSImVD/n\nz4y52Hmu05J5rbZtjogTq8/w7XWv1Zx2n5AYxQmMM6ldWXI3cEG769mHcbya2p9stwO3VY8zqR1r\nvA74DfBjYFrVP4BLq3H/Guive613A4PV413tHluT4z+FP1wtcyS1/2gHgW8C46r28dX6YLX9yLrn\nX1B9FqsZ4VUEbRjrscBANdffo3ZVRNHzDHwKuBO4A/gatSteippn4Epq5xS2U/sL7T2tnFegv/r8\n7gb+jWEn5Rs9/IaqJBWo0w7LSJKaYLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklSg/weX\n5RUwoExH6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1153c7b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059.0\n"
     ]
    }
   ],
   "source": [
    "print sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
